// Note: all terms referring to size are in bytes, unless suffixed with _mb, _tb, etc.
// Note: all terms referring to time are in seconds, unless suffixed with _mins, _days, etc.

Rig:
  describe(rig_sectors_min, "Minimum sectors require to mine")

  describe(rig_storage_latency, "Random Access latency on the storage medium (honest)")
  describe(rig_storage_parallelization, "Storage Units that can be accessed in parallel (honest)")
  describe(rig_storage_min, "Minimum storage required to mine", bytes)
  describe(rig_storage_min_tb, "Minimum storage required to mine", tib)
  describe(rig_storage_parallelization_min, "Minimum storage units that must be accessed in parallel in order to mine (honest)")
  describe(rig_storage_unit_max, "Maximum storage capacity for a storage unit", bytes)
  describe(rig_storage_unit_max_tb, "Maximum storage capacity for a storage unit", tib)

  describe(rig_malicious_cost_per_year, "Cost of the malicious computing rig per year")
  describe(rig_cores, "Number of CPU cores")
  describe(rig_storage_read_mbs, "Sequential read speed (MiB/s)")

  rig_storage_min = rig_storage_min_tb * 1024 * 1024 * 1024 * 1024
  rig_sectors_min = rig_storage_min / sector_size
  rig_storage_parallelization_min = post_data_access / polling_time

  rig_storage_unit_max = rig_storage_min / rig_storage_parallelization_min
  rig_storage_unit_max_tb = rig_storage_unit_max / (1024 * 1024 * 1024 * 1024)

Crypto:
  describe(snark_size, "Size of a SNARK proof", bytes)
  describe(hashing_amax, "Factor of difference between stock CPU hashing and the fastest hashing")

  describe(merkle_tree_datahash_constraints, "Constraints for hashing two nodes in CommD")
  describe(merkle_tree_datahash_time, "Time to hash two nodes in CommD")
  describe(merkle_tree_hash_constraints, "Constraints for hashing two nodes in CommR")
  describe(merkle_tree_hash_time, "Time to hash two nodes in CommR")
  describe(column_leaf_hash_constraints, "Constraints for hashing a single row in a column", secs)
  describe(column_leaf_hash_time, "Time to compute the hash of a single row in a column", secs)

Utils:
  describe(cid_size, "Size of a CID (bytes)")
  describe(secs_in_month, "Number of seconds in a month")
  describe(b_in_gb, "Number of bytes in a GiB")

  b_in_gb = (1024 * 1024 * 1024)
  secs_in_month = (24 * 30 * 60 * 60)
  cid_size = 32 + 1 + 1 + 1

ProofOfSpacetime:
  // post_vanilla_size = node_size * (log2(nodes) + 1) * post_challenges
  Security:
    describe(post_challenges, "Number of challenges in a PoRep proof")

  Time:
    describe(post_time, "Time to generate a PoSt Proof on a single core CPU")
    describe(post_time_parallel, "Parallel time to generate a PoSt proof on RIG_CORES CPU cores")
    describe(post_snark_time, "Time to generate a PoSt SNARK")

    post_time = post_proof_gen
    post_time_parallel = post_proof_gen_parallel

    ProofGeneration:
      describe(post_proof_gen, "Time to generate a PoSt Proof (including SNARKs, generating witnesses and reading from storage the challenged leaves)")
      describe(post_proof_gen_parallel, "Time to generate a PoSt Proof (including SNARKs, generating witnesses and reading from storage the challenged leaves) in parallel")
      describe(post_inclusions_time, "Time to regenerate and to read the merkle tree inclusion proofs for the challenged nodes from memory")

      post_proof_gen = post_snark_time + post_inclusions_time + post_data_access 
      post_proof_gen = post_snark_time_parallel + post_inclusions_time_parallel + post_data_access_parallel

    DiskLatency:
      describe(post_data_access, "Time to retrieve from the storage unit the data (excluding Merkle tee) for PoSt")
      describe(post_data_access_parallel, "Time to retrieve from the storage unit the data (excluding Merkle tee) in parallel for PoSt")

      post_data_access_parallel = post_data_access / rig_storage_parallelization

      DataAccess (Wrapping):
        post_data_access = post_challenges * rig_storage_latency

      DataAccess (WrappingVariant):
        describe(wrapper_kdf_time, "Time to generate a node on the wrapper layer", secs)
        describe(wrapper_challenged_nodes_parents, "Number of parents of all the challenged nodes on the wrapper layer in PoSt")
        describe(wrapper_challenged_nodes_parents_read, "Time to retrieve from storage medium all the parents of the challenged nodes on the wrapper layer in PoSt", secs)
        describe(wrapper_challenged_nodes_parents_regen, "Time to re-generate all the challenged nodes on the wrapper layer from the window outputs", secs)

        wrapper_kdf_time = kdf_time * wrapper_parents
        wrapper_challenged_nodes_parents = post_challenges * wrapper_parents
        wrapper_challenged_nodes_parents_read = wrapper_challenged_nodes_parents * rig_storage_latency
        wrapper_challenged_nodes_parents_regen = wrapper_kdf_time * post_challenges

        post_data_access = wrapper_challenged_nodes_parents_regen + wrapper_challenged_nodes_parents_read 

      DataAccess (StackedReplicas):
        DataLayout (StackedReplicaDistributed):
          post_data_access = post_challenges * windows * rig_storage_latency
        DataLayout (StackedReplicaSequential):
          post_data_access = post_challenges * node_size_mb * rig_storage_read_mbs

    // TODO: add MerkleTreeCaching for ColumnR
    MerkleTreeCaching:
      Storage:
        describe(post_mtree_cached, "Size of the cached parts of the replica merkle tree (bytes)")
        describe(post_mtree_cached_hashnodes, "Number of intermediary hash nodes cached of the replica merkle tree")
        describe(post_mtree_cached_gb, "POST_MTREE_CACHED in GiB")
        describe(post_mtree_overhead, "Fraction of extra storage a miner will have to use to cache the replica merkle tree")
        describe(post_mtree_layers_cached, "Number of layers of the replica merkle tree being cached")
        describe(post_mtree_layers_deleted, "Number of layers of the replica merkle tree not being cached")

        post_mtree_cached = post_mtree_cached_hashnodes * node_size
        post_mtree_cached_gb = post_mtree_cached / (1024 * 1024 * 1024)
        post_mtree_overhead = post_mtree_cached / sector_size
        post_mtree_cached_hashnodes = expt(2, post_mtree_layers_cached) - 1
        post_mtree_layers_deleted = commr_tree_depth - post_mtree_layers_cached

      Time:
        // Note: this assumes reading the cached parts of the merkle tree instantaneous

        post_inclusions_time = post_mtree_challenges_regen + post_mtree_challenges_read

        Regeneration:
          describe(post_mtree_regen_hashnodes, "Number of intermediary nodes to be regenerated for a single replica merkle tree inclusion proof in PoSt")
          describe(post_mtree_regen_challenge_time, "Time to generate the missing intermediary hash nodes of the replica merkle tree inclusion proof for a single challenge in PoSt")
          describe(post_mtree_challenges_regen, "Same as post_mtree_regen_challenge_time but for all the PoSt challenges")

          post_mtree_regen_hashnodes = expt(2, post_mtree_layers_deleted + 1) - 1
          post_mtree_regen_challenge_time = (post_mtree_regen_hashnodes * 2) * merkle_tree_hash_time
          post_mtree_challenges_regen = post_mtree_regen_challenge_time * post_challenges

        Access:
          describe(post_mtree_challenge_read, "Time to sequentially read the replica leaves needed to regenerate the missing layers of the merkle tree for a single challenged node in PoSt")
          describe(post_mtree_challenges_read, "Same as post_mtree_challenge_read but for all the PoSt challenges")

          // TODO: this is using random access, we should use seq access instead
          post_mtree_challenge_read = post_mtree_regen_hashnodes * rig_storage_latency
          post_mtree_challenges_read = post_mtree_challenge_read * post_challenges
          // TODO: log2(nodes) > post_mtree_layers_cached

  Time (NoWrapper):
    //rig_storage_latency_throughput_kb = rig_storage_latency_throughput / 1024
    //response_honest_min = (post_challenges * challenged_sectors_percentage * rig_sectors_min * rig_storage_latency *windows) / rig_storage_latency_throughput

  SNARK:
    describe(inclusion_proof, "Constraints for checking an inclusion proof in a binary merkle tree as large as the sector")
    describe(post_inclusion_proofs, "Constraints for checking all inclusion proofs (in PoSt)")
    describe(ticket_proofs, "Constraints for checking the partial ticket (in PoSt)")
    describe(ticket_constraints, "Constraints of a partial ticket per challenged node (in PoSt)")
    describe(post_snark_constraints, "Constraints for a PoSt")
    describe(post_snark_partitions, "Number of partitions of a PoSt proof")
    describe(post_snark_size, "Size of a PoSt proof (bytes)")
    describe(post_snark_size_kb, "Size of a Post proof (KiB)")
    describe(post_snark_partition_constraints, "Number of constraints for a PoSt partition")

    inclusion_proof = merkle_tree_hash_constraints * log2(nodes)
    post_inclusion_proofs = inclusion_proof * post_challenges
    ticket_proofs = ticket_constraints * post_challenges

    post_snark_constraints = ticket_proofs + post_inclusion_proofs
    post_snark_partitions = post_snark_constraints / post_snark_partition_constraints
    post_snark_size = post_snark_partitions * snark_size
    post_snark_size_kb = post_snark_size / 1024

ProofOfReplication:
  Security:
    describe(porep_challenges, "Number of challenges in a PoRep proof")
    describe(porep_lambda, "Bits of security of a PoRep proof")
    describe(spacegap, "Maximum difference in storage between an honest prover and a maliciois prover")

    porep_challenges = -porep_lambda / (log2(1-spacegap/4))

  Graph:
    describe(nodes, "Number of nodes in a sector")
    describe(sector_size, "Size of a sector", bytes)
    describe(sector_size_mb, "Size of a sector", mib)
    describe(sector_size_gb, "Size of a sector", gib)

    describe(window_nodes, "Number of nodes in a window")
    describe(window_size, "Size of a window (bytes)")
    describe(window_size_mb, "Size of a window (MiB)")
    describe(windows, "Number of windows in a sector")

    describe(graph_parents, "Number of parents of a node in the PoRep window graph")
    describe(drg_parents, "Number of DRG parents of a node in the PoRep window graph")
    describe(expander_parents, "Number of Chung Expander parents of a node in the PoRep window graph")
    describe(drg_layers, "Number of layers in the DRG")
    describe(wrapper_parents, "Number of parents in the wrapper layer")

    describe(node_size, "Size of a node", bytes)
    describe(node_size_mb, "Size of a node", mib)
    describe(node_size_gb, "Size of a node", gib)
    describe(kdf_hash_size, "Size of the input to the KDF hash (bytes)")
    describe(kdf_hash_size_gb, "Size of the input to the KDF hash (GiB)")

    sector_size = sector_size_gb * b_in_gb
    sector_size_mb = sector_size / (1024 * 1024)
    graph_parents = drg_parents + expander_parents
    nodes = sector_size / node_size
    node_size_mb = node_size / (1024 * 1024)
    node_size_gb = node_size / b_in_gb 
    kdf_hash_size_gb = graph_parents * node_size_gb
    kdf_hash_size = kdf_hash_size_gb * b_in_gb

    window_size = window_size_mb * 1024 * 1024
    windows = sector_size / window_size
    window_nodes = nodes / windows

  Time:
    describe(porep_time, "Time to seal a sector (PoRep proof)", secs)
    describe(porep_time_parallel, "Time to seal a sector (PoRep proof) with parallelization", secs)
    describe(porep_proof_gen, "Time to generate a PoRep proof (SNARK, witness generation, loading data from storage medium)", secs)
    describe(porep_proof_gen_parallel, "Time to generate a PoRep proof (SNARK, witness generation, loading data from storage medium) in parallel", secs)

    porep_time = porep_proof_gen + porep_commit_time + encoding_time
    porep_time_parallel = porep_proof_gen_parallel + porep_commit_time_parallel + encoding_time_parallel

  Commitment:
    describe(window_comm_tree_time, "Time to generate a merkle tree of stacked windows")
    describe(window_comm_leaves_time, "Time to commit columns of a window")

    describe(commit_size, "Size of a commitment (bytes)")
    describe(commr_size, "Size of CommR (bytes)")
    describe(commd_size, "Size of CommD (bytes)")
    describe(commrlast_size, "Size of CommRLast (bytes)")

    describe(commr_tree_depth, "Number of layers of the merkle tree with root hash CommR")

    describe(seal_onchain_commitments_size, "Size of the seal commitments", bytes)
    describe(porep_commit_time, "Time to generate CommC, CommR and CommQ", secs)
    describe(porep_commit_time_mins, "Time to generate CommC, CommR and CommQ", mins)
    describe(porep_commit_time_parallel, "Time to generate CommC, CommR and CommQ in parallel", secs)


    describe(commr_time, "Time to generate CommR (seconds)")
    describe(commd_time, "Time to generate CommD (seconds)")
    describe(commc_time, "Time to generate CommC (seconds)")

    commit_size = cid_size
    // TODO: should be missing *2
    window_comm_tree_time = merkle_tree_hash_time * window_nodes
    window_comm_leaves_time = column_leaf_hash_time * windows
    commr_size = commit_size
    commd_size = commit_size
    commrlast_size = commit_size

    seal_onchain_commitments_size = commd_size + commr_size
    // seal_commitment_time = commr_time

    commd_time = merkle_tree_datahash_time * nodes

    // TODO commr might be twice depending on CommQ
    porep_commit_time = commr_time + commc_time
    porep_commit_time_mins = porep_commit_time / 60

    CommR (VectorR):
      commr_time = merkle_tree_hash_time * nodes
      commr_tree_depth = log2(nodes) 

    CommR (ColumnR):
      commr_time = window_comm_tree_time + window_comm_leaves_time 
      commr_tree_depth = log2(window_nodes)

    CommC:
      describe(commc_tree_time, "Time to generate the merkle tree of CommC (after the leaves have been computed)", secs)
      commc_tree_time = (window_comm_tree_time + window_comm_leaves_time) * drg_layers 

  Encoding:
    describe(encoding_time, "Time to encode a sector", secs)
    describe(encoding_time_parallel, "Time to encode a sector in parallel", secs)
    describe(encoding_time_asic, "Time to encode a sector with an ASIC", secs)
    describe(encoding_time_parallel_asic, "Time to encode a sector in parallel with ASICs", secs)
    describe(encoding_window_time, "Time to encode a window", secs)
    describe(encoding_window_time_parallel, "Time to encode a window in parallel", secs)
    describe(encoding_window_time_asic, "Time to encode a window with an ASIC", secs)
    describe(encoding_window_time_parallel_asic, "Time to encode a window in parallel with ASICs", secs)
    describe(kdf_time, "Time to compute a KDF", secs)

    encoding_time = encoding_window_time * windows
    encoding_time_parallel = encoding_time / rig_cores

    encoding_window_time = (window_nodes * graph_parents) * kdf_time * drg_layers
    encoding_window_time_parallel = encoding_window_time / rig_cores

    Asic:
      encoding_time_parallel_asic = encoding_time_parallel / hashing_amax
      encoding_time_asic = encoding_time / hashing_amax
      encoding_window_time_asic = encoding_window_time / hashing_amax
      encoding_window_time_parallel_asic = encoding_window_time_parallel / hashing_amax

  Retrieval:
    describe(window_read_time, "Time to read a window from storage unit", secs)
    describe(window_read_time_parallel, "Time to read a window from storage unit in parallel", secs)
    describe(decoding_time, "Time to decode a sector", secs)
    describe(decoding_time_parallel, "Time to decode a sector in parallel", secs)

    Decoding (StackedReplicas):
      // Decoding in StackedReplicas: read the window output and decode it
      window_read_time = window_size_mb / rig_storage_read_mbs
      window_read_time_parallel = window_read_time / rig_storage_parallelization
      decoding_time = encoding_window_time + window_read_time
      decoding_time_parallel = encoding_window_time_parallel + window_read_time_parallel

    Decoding (WrappingVariant):
      // Decoding in WrappingVariant: read the window output and decode them
      window_read_time = window_size_mb / rig_storage_read_mbs
      window_read_time_parallel = window_read_time / rig_storage_parallelization
      decoding_time = encoding_window_time + window_read_time
      decoding_time_parallel = encoding_window_time_parallel + window_read_time_parallel

    Decoding (Wrapping):
      // Decoding in Wrapping: read all the window outputs and decode them
      window_read_time = sector_size_mb / rig_storage_read_mbs
      window_read_time_parallel = window_read_time / rig_storage_parallelization
      decoding_time = encoding_time + window_read_time
      decoding_time_parallel = encoding_time_parallel + window_read_time_parallel

  SNARK:
    describe(porep_snark_constraints, "Constraints for a PoRep")
    describe(porep_snark_partitions, "Number of partitions of a PoRep proof")
    describe(porep_snark_size, "Size of a PoRep proof (bytes)")
    describe(porep_snark_size_kb, "Size of a PoRep proof (KiB)")
    describe(porep_snark_partition_constraints, "Number of constraints for a PoRep partition")
    describe(kdf_constraints, "Constraints of KDF per node")

    describe(labeling_proof, "Constraints to check labeling of a single node")
    describe(labeling_proofs, "Constraints to check labeling of a window")
    describe(window_labeling_proof, "Constraints to check correct labeling for all windows")
    describe(data_inclusion, "Constraints to check a single inclusion in CommD")
    describe(data_inclusions, "Constraints to check all the inclusions required in a window in CommD")
    describe(window_data_inclusions,  "Constraints to check all the inclusions required for all windows in CommD")
    describe(replica_inclusion, "Constraints to check a single inclusion in CommR")
    describe(replica_inclusions, "Constraints to check all the inclusions required in a window in CommR")
    describe(window_replica_inclusions,  "Constraints to check all the inclusions required for all windows in CommR")
    describe(window_inclusion, "Constraints to check a single inclusion proof in a window")
    describe(window_inclusions, "Constraints to check all the required inclusion proofs in a window")
    describe(column_leaf, "Constraints to check a single leaf generation from a column in a window")
    describe(column_leaves, "Constraints to check all the leaves generation required in a window")
    describe(window_column_leaves, "Constraints to check all the leaves generation required for all windows")

    // D
    // Data layer committed with merkle tree
    data_inclusion = merkle_tree_datahash_constraints * log2(nodes)
    data_inclusions = data_inclusion * porep_challenges
    window_data_inclusions = data_inclusion * porep_challenges * windows

    // C
    window_inclusion = merkle_tree_hash_constraints * log2(window_nodes)
    window_inclusions =  window_inclusion * porep_challenges * (graph_parents + 1)
    column_leaf = column_leaf_hash_constraints * drg_layers
    column_leaves = column_leaf * porep_challenges * (graph_parents + 1)
    window_column_leaves = column_leaves * windows
    labeling_proof = kdf_constraints * graph_parents
    labeling_proofs = labeling_proof * drg_layers * porep_challenges
    window_labeling_proof = labeling_proofs * windows

    // Q and R
    replica_inclusion = merkle_tree_hash_constraints * log2(nodes)
    replica_inclusions = replica_inclusion * porep_challenges
    window_replica_inclusions = replica_inclusions * windows

    porep_snark_partitions = porep_snark_constraints / porep_snark_partition_constraints
    porep_snark_size = porep_snark_partitions * snark_size
    porep_snark_size_kb = porep_snark_size / 1024

    Constraints (WindowEncoding):
      porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + window_data_inclusions + window_replica_inclusions*2

    Constraints (SectorEncoding):
      porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + data_inclusions + replica_inclusions*2

    Constraints (Wrapping):
      porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + data_inclusions + replica_inclusions*2

    // Column comitting data or replica. 

Rationality:
  describe(breakeven_nodes, "number of nodes / sector / challenge period after which it's cheaper to store the entire sector, than re-encode those")
  describe(polling_time, "Time between two PoSt proofs")
  describe(challenge_periods_per_month, "Number of challenge periods in a month")
  describe(regeneration_fraction, "Fraction of nodes of a sector for which re-encoding them every `polling_time` costs as much as storing them")

  breakeven_nodes = ((cost_gb_per_month *  sector_size_gb) / (cost_node_encoding * challenge_periods_per_month))
  challenge_periods_per_month = secs_in_month / polling_time
  regeneration_fraction = breakeven_nodes / nodes

  Costs:
    Compute:
      describe(cost_node_encoding, "Cost of encoding a single node")
      describe(nodes_per_second, "Nodes encoded per second using the fastest hardware")
      describe(nodes_per_year, "Nodes encoded per year using the fastest hardware")
      describe(hash_gb_per_second, "GiB that can be hashed per second")

      nodes_per_second = hash_gb_per_second / kdf_hash_size_gb
      nodes_per_year = nodes_per_second * (60 * 60 * 24 * 365)
      cost_node_encoding = rig_malicious_cost_per_year / nodes_per_year 

    Storage:
      describe(cost_gb_per_month, "Cost of storing 1GiB per month")
      describe(cost_storage_gbs, "Cost of storing 1GiB per second")
      describe(cost_storage_sector_polling_time, "Cost of storage for `polling_time`")
      describe(extra_storage_time, "Time for which an honest user must be storing a file beyond the challenge period")

      cost_storage_gbs = cost_gb_per_month / secs_in_month 
      cost_storage_sector_polling_time = cost_storage_gbs * (sector_size_gb * (polling_time + extra_storage_time))

// Throughput:
//   num_parallel_encoding = rig_ram_gb * rig_size / sector_size_gb
//   rig_cores = num_parallel_encoding
//   seal_throughput = (sector_size_gb * num_parallel_encoding) / encoding_time