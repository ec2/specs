// Note: all terms referring to size are in bytes, unless suffixed with _mb, _tb, etc.
// Note: all terms referring to time are in seconds, unless suffixed with _mins, _days, etc.

Rig:
  describe(rig_storage_latency, "Random Access latency on the storage medium (honest)")
  describe(rig_storage_parallelization, "Storage Units that can be accessed in parallel (honest)")
  describe(rig_storage_min, "Minimum storage required to mine (bytes)")
  describe(rig_sectors_min, "Minimum sectors require to mine")
  describe(rig_storage_parallelization_min, "Minimum storage units that must be accessed in parallel in order to mine (honest)")

  rig_storage_min = rig_storage_min_tb * 1024 * 1024 * 1024 * 1024
  rig_sectors_min = rig_storage_min / sector_size
  rig_storage_parallelization_min = post_leaves_access / polling_time

  rig_storage_unit_min = rig_storage_min / rig_storage_parallelization_min
  rig_storage_unit_min_tb = rig_storage_unit_min / (1024 * 1024 * 1024 * 1024)

Crypto:
  describe(snark_size, "Size of a SNARK proof (bytes)")

Utils:
  describe(cid_size, "Size of a CID (bytes)")

  b_in_gb = (1024 * 1024 * 1024)
  secs_in_month = (24 * 30 * 60 * 60)
  cid_size = 32 + 1 + 1 + 1

ProofOfSpacetime:
  // post_vanilla_size = node_size * (log2(nodes) + 1) * post_challenges
  Security:
    describe(post_challenges, "Number of challenges in a PoRep proof")

  Time:
    describe(post_time, "Time to generate a PoSt Proof on a single core CPU")
    describe(post_time_parallel, "Parallel time to generate a PoSt proof on RIG_CORES CPU cores")

    post_time = post_leaves_access + post_proof_gen
    post_time_parallel = post_leaves_access_parallel + post_proof_gen_parallel

    DiskLatency:
      describe(post_leaves_access, "Time to retrieve all challenged nodes during PoSt")
      describe(post_leaves_access_parallel, "Parallel time to retrieve all challenged nodes during PoSt")

      post_leaves_access = post_challenges * rig_storage_latency
      post_leaves_access_parallel = post_leaves_access / rig_storage_parallelization

    // TODO: add MerkleTreeCaching for ColumnR
    MerkleTreeCaching:
      Storage:
        describe(post_mtree_cached, "Size of the cached parts of the replica merkle tree (bytes)")
        describe(post_mtree_cached_hashnodes, "Number of intermediary hash nodes cached of the replica merkle tree")
        describe(post_mtree_cached_gb, "POST_MTREE_CACHED in GiB")
        describe(post_mtree_overhead, "Fraction of extra storage a miner will have to use to cache the replica merkle tree")
        describe(post_mtree_layers_cached, "Number of layers of the replica merkle tree being cached")
        describe(post_mtree_layers_deleted, "Number of layers of the replica merkle tree not being cached")

        post_mtree_cached = post_mtree_cached_hashnodes * node_size
        post_mtree_cached_gb = post_mtree_cached / (1024 * 1024 * 1024)
        post_mtree_overhead = post_mtree_cached / sector_size
        post_mtree_cached_hashnodes = expt(2, post_mtree_layers_cached) - 1
        post_mtree_layers_deleted = commr_tree_depth - post_mtree_layers_cached

      Time:
        // Note: this assumes reading the cached parts of the merkle tree instantaneous
        describe(post_witness_gen, "Time to regenerate and to read the merkle tree inclusion proofs for the challenged nodes from memory")

        post_witness_gen = post_mtree_regen_challenges_time + post_mtree_access_challenges_time

        Regeneration:
          describe(post_mtree_regen_hashnodes, "Number of intermediary nodes to be regenerated for a single replica merkle tree inclusion proof in PoSt")
          describe(post_mtree_regen_challenge_time, "Time to generate the missing intermediary hash nodes of the replica merkle tree inclusion proof for a single challenge in PoSt")
          describe(post_mtree_regen_challenges_time, "Same as post_mtree_regen_challenge_time but for all the PoSt challenges")

          post_mtree_regen_hashnodes = expt(2, post_mtree_layers_deleted + 1) - 1
          post_mtree_regen_challenge_time = (post_mtree_regen_hashnodes * 2) * merkle_tree_hash_time
          post_mtree_regen_challenges_time = post_mtree_regen_challenge_time * post_challenges

        Access:
          describe(post_mtree_access_challenge_time, "Time to sequentially read the replica leaves needed to regenerate the missing layers of the merkle tree for a single challenged node in PoSt")
          describe(post_mtree_access_challenges_time, "Same as post_mtree_access_challenge_time but for all the PoSt challenges")

          // TODO: this is using random access, we should use seq access instead
          post_mtree_access_challenge_time = post_mtree_regen_hashnodes * rig_storage_latency
          post_mtree_access_challenges_time = post_mtree_access_challenge_time * post_challenges
          // TODO: log2(nodes) > post_mtree_layers_cached

    ProofGeneration:
      describe(post_proof_gen, "Time to generate a PoSt Proof (including SNARKs, generating witnesses and reading from storage the challenged leaves)")

      post_proof_gen = post_snark_time + post_witness_gen 

  Time (NoWrapper):
    //rig_storage_latency_throughput_kb = rig_storage_latency_throughput / 1024
    //response_honest_min = (post_challenges * challenged_sectors_percentage * rig_sectors_min * rig_storage_latency *windows) / rig_storage_latency_throughput

  SNARK:
    describe(inclusion_proof, "Constraints for checking an inclusion proof in a binary merkle tree as large as the sector")
    describe(post_inclusion_proofs, "Constraints for checking all inclusion proofs (in PoSt)")
    describe(ticket_proofs, "Constraints for checking the partial ticket (in PoSt)")
    describe(ticket_constraints, "Constraints of a partial ticket per challenged node (in PoSt)")
    describe(post_snark_constraints, "Constraints for a PoSt")
    describe(post_snark_partitions, "Number of partitions of a PoSt proof")
    describe(post_snark_size, "Size of a PoSt proof (bytes)")
    describe(post_snark_size_kb, "Size of a Post proof (KiB)")
    describe(post_snark_partition_constraints, "Number of constraints for a PoSt partition")

    inclusion_proof = merkle_tree_hash_constraints * log2(nodes)
    post_inclusion_proofs = inclusion_proof * post_challenges
    ticket_proofs = ticket_constraints * post_challenges

    post_snark_constraints = ticket_proofs + post_inclusion_proofs
    post_snark_partitions = post_snark_constraints / post_snark_partition_constraints
    post_snark_size = post_snark_partitions * snark_size
    post_snark_size_kb = post_snark_size / 1024

ProofOfReplication:
  Security:
    describe(porep_challenges, "Number of challenges in a PoRep proof")
    describe(porep_lambda, "Bits of security of a PoRep proof")

    porep_challenges = -porep_lambda / (log2(1-spacegap/4))

  Graph:
    describe(nodes, "Number of nodes in a sector")
    describe(sector_size, "Size of a sector (bytes)")
    describe(sector_size_gb, "Size of a sector (GiB)")

    describe(window_nodes, "Number of nodes in a window")
    describe(window_size, "Size of a window (bytes)")
    describe(window_size_mb, "Size of a window (MiB)")

    describe(graph_parents, "Number of parents of a node in the PoRep window graph")
    describe(drg_parents, "Number of DRG parents of a node in the PoRep window graph")
    describe(expander_parents, "Number of Chung Expander parents of a node in the PoRep window graph")

    describe(node_size, "Size of a node (bytes)")
    describe(node_size_gb, "Size of a node (GiB)")
    describe(kdf_hash_size, "Size of the input to the KDF hash (bytes)")
    describe(kdf_hash_size_gb, "Size of the input to the KDF hash (GiB)")

    sector_size = sector_size_gb * b_in_gb
    graph_parents = drg_parents + expander_parents
    nodes = sector_size / node_size 
    node_size_gb = node_size / b_in_gb 
    kdf_hash_size_gb = graph_parents * node_size_gb
    kdf_hash_size = kdf_hash_size_gb * b_in_gb

    window_size = window_size_mb * 1024 * 1024
    windows = sector_size / window_size
    window_nodes = nodes / windows

  Time:
    describe(porep_time, "Time to seal a sector (PoRep proof)")
    describe(porep_time_parallel, "Time to seal a sector (PoRep proof) with parallelization")

    porep_time = porep_proof_gen + porep_commit_time + encoding_time
    porep_time_parallel = porel_proof_gen_parallel + porep_commit_time_parallel + encoding_time_parallel

  Commitment:
    describe(window_comm_tree_time, "Time to generate a merkle tree of stacked windows")
    describe(window_comm_leaves_time, "Time to commit columns of a window")

    describe(commit_size, "Size of a commitment (bytes)")
    describe(commr_size, "Size of CommR (bytes)")
    describe(commd_size, "Size of CommD (bytes)")
    describe(commrlast_size, "Size of CommRLast (bytes)")

    describe(seal_onchain_commitments_size, "Size of the seal commitments", bytes)
    describe(porep_commit_time, "Time to generate CommC, CommR and CommQ", secs)
    describe(porep_commit_time_mins, "Time to generate CommC, CommR and CommQ", mins)

    describe(commr_time, "Time to generate CommR (seconds)")
    describe(commd_time, "Time to generate CommD (seconds)")

    commit_size = cid_size
    // TODO: should be missing *2
    window_comm_tree_time = merkle_tree_hash_time * window_nodes
    window_comm_leaves_time = leaf_hash_time * windows
    commr_size = commit_size
    commd_size = commit_size
    commrlast_size = commit_size

    seal_onchain_commitments_size = commd_size + commr_size
    // seal_commitment_time = commr_time

    commd_time = merkle_tree_datahash_time * nodes

    // TODO commr might be twice depending on CommQ
    porep_commit_time = commr_time + commc_time
    porep_commit_time_mins = porep_commit_time / 60

    CommR (VectorR):
      commr_time = merkle_tree_hash_time * nodes
      commr_tree_depth = log2(nodes) 

    CommR (ColumnR):
      commr_time = window_comm_tree_time + window_comm_leaves_time 
      commr_tree_depth = log2(window_nodes)

    CommC:
      commc_tree_time = (window_comm_tree_time + window_comm_leaves_time) * drg_layers 

  Encoding:
    encoding_window_time = (window_nodes * graph_parents) * kdf_time * drg_layers
    encoding_time = encoding_window_time * windows
    decoding_time = encoding_window_time

    encoding_time_parallel = encoding_time / rig_cores
    window_time_parallel = encoding_window_time / rig_cores

    Asic:
      encoding_time_parallel_asic = encoding_time_parallel / hashing_amax
      encoding_time_asic = encoding_time / hashing_amax
      encoding_window_time_asic = encoding_window_time / hashing_amax
      encoding_window_time_parallel_asic = encoding_window_time_parallel / hashing_amax

  Retrieval:
    // window_read = rig_storage_read_mbs * window_size_mb

  SNARK:
    // D
    // Data layer committed with merkle tree
    data_inclusion = merkle_tree_datahash_constraints * log2(nodes)
    data_inclusions = data_inclusion * porep_challenges
    window_data_inclusions = data_inclusion * porep_challenges * windows

    // C
    window_inclusion = merkle_tree_hash_constraints * log2(window_nodes)
    window_inclusions =  window_inclusion * porep_challenges * (graph_parents + 1)
    column_leaf = leaf_hash_constraints * drg_layers
    column_leaves = column_leaf * porep_challenges * (graph_parents + 1)
    window_column_leaves = column_leaves * windows
    labeling_proof = kdf_constraints * graph_parents
    labeling_proofs = labeling_proof * drg_layers * porep_challenges
    window_labeling_proof = labeling_proofs * windows

    // Q and R
    replica_inclusion = merkle_tree_hash_constraints * log2(nodes)
    replica_inclusions = replica_inclusion * porep_challenges
    window_replica_inclusions = replica_inclusions * windows

    porep_snark_partitions = porep_snark_constraints / porep_max_snark_constraints
    porep_snark_size = porep_snark_partitions * snark_size
    porep_snark_size_kb = porep_snark_size / 1024

    Constraints (WindowEncoding):
      porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + window_data_inclusions + window_replica_inclusions*2

    Constraints (SectorEncoding):
      porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + data_inclusions + replica_inclusions*2

    Constraints (Wrapping):
      porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + data_inclusions + replica_inclusions*2

    // Column comitting data or replica. 

Rationality:
  describe(breakeven_nodes, "number of nodes / sector / challenge period after which it's cheaper to pay store the entire sector, than re-encode those")
  breakeven_nodes = ((cost_gb_per_month *  sector_size_gb) / (cost_hash_node * challenge_periods_per_month))
  challenge_periods_per_month = secs_in_month / polling_time
  regeneration_percentage = breakeven_nodes / nodes

  Costs:
    Compute:
      nodes_per_second = hash_gb_per_second / kdf_hash_size_gb
      nodes_per_year = nodes_per_second * (60 * 60 * 24 * 365)
      cost_hash_node = rig_cost_per_year / nodes_per_year 

    Storage:
      cost_storage_gbs = cost_gb_per_month / secs_in_month 
      cost_storage_sector_polling_time = cost_storage_gbs * (sector_size_gb * (polling_time + extra_storage_time))

// Throughput:
//   num_parallel_encoding = rig_ram_gb * rig_size / sector_size_gb
//   rig_cores = num_parallel_encoding
//   seal_throughput = (sector_size_gb * num_parallel_encoding) / encoding_time