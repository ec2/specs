// If you wish to make an apple pie from scratch, you must first uber-calc the universe.

// TODO:
// - Time cost per constraint
Rig:
  describe(rig_sectors_min, "Minimum sectors require to mine")

  describe(rig_storage_latency, "Random Access latency on the storage medium (honest)", secs)
  describe(rig_storage_parallelization, "Storage Units that can be accessed in parallel (honest)")
  describe(rig_storage_min, "Minimum storage required to mine", bytes)
  describe(rig_storage_min_tib, "Minimum storage required to mine", tib)
  describe(rig_storage_parallelization_min, "Minimum storage units that must be accessed in parallel in order to mine (honest)")
  describe(rig_storage_unit_max, "Maximum storage capacity for a storage unit", bytes)
  describe(rig_storage_unit_max_tib, "Maximum storage capacity for a storage unit", tib)

  describe(rig_malicious_cost_per_year, "Cost of the malicious computing rig per year")
  describe(rig_cores, "Number of CPU cores")
  describe(rig_storage_read_mbs, "Sequential read speed (MB/s)")
  describe(rig_storage_read_mibs, "Sequential read speed (MiB/s)")

  rig_storage_read_mibs = rig_storage_read_mbs * to_i
  rig_storage_min = rig_storage_min_tib * b_in_tib
  rig_sectors_min = rig_storage_min / sector_size
  rig_storage_parallelization_min = post_data_access / polling_time

  rig_storage_unit_max = rig_storage_min / rig_storage_parallelization_min
  rig_storage_unit_max_tib = rig_storage_unit_max / b_in_tib 

Crypto:
  describe(snark_size, "Size of a SNARK proof", bytes)
  describe(snark_constraint_time, "Time of proving a SNARK constraint", secs)
  describe(hashing_amax, "Factor of difference between stock CPU hashing and the fastest hashing")

  describe(merkle_tree_datahash_constraints, "Constraints for hashing two nodes in CommD")
  describe(merkle_tree_datahash_time, "Time to hash two nodes in CommD", secs)
  describe(merkle_tree_hash_constraints, "Constraints for hashing two nodes in CommR")
  describe(merkle_tree_hash_time, "Time to hash two nodes in CommR", secs)
  describe(column_leaf_hash_constraints, "Constraints for hashing a single row in a column", secs)
  describe(column_leaf_hash_time, "Time to compute the hash of a single row in a column", secs)

Utils:
  describe(cid_size, "Size of a CID", bytes)
  describe(secs_in_month, "Number of seconds in a month")
  describe(secs_in_year, "Number of seconds in a year")
  describe(b_in_eib, "Number of bytes in a EiB")
  describe(b_in_pib, "Number of bytes in a PiB")
  describe(b_in_tib, "Number of bytes in a TiB")
  describe(b_in_gib, "Number of bytes in a GiB")
  describe(b_in_mib, "Number of bytes in a MiB")
  describe(b_in_kib, "Number of bytes in a KiB")
  describe(to_i, "Ratio between GB and GiB")

  b_in_tib = (1024 * 1024 * 1024 * 1024 * 1024 * 1024)
  b_in_pib = (1024 * 1024 * 1024 * 1024 * 1024)
  b_in_tib = (1024 * 1024 * 1024 * 1024)
  b_in_gib = (1024 * 1024 * 1024)
  b_in_mib = (1024 * 1024)
  b_in_kib = 1024
  secs_in_month = (24 * 30 * 60 * 60)
  secs_in_year = (24 * 30 * 60 * 60 * 365)
  cid_size = 32 + 1 + 1 + 1
  to_i = 1024/1000

ProofOfSpacetime:
  // post_vanilla_size = node_size * (log2(nodes) + 1) * post_challenges
  Security:
    describe(post_challenges, "Number of challenges in a PoSt proof")
    describe(post_lambda, "Bits of security of a PoSt proof")
    describe(post_soundness, "Probability of an adversary to succeed in generating a convincing PoSt proof")
    post_soundness = expt(2, -post_lambda)

  Size:
    describe(post_proof_size, "Size of a PoSt proof")
    post_proof_size = post_snark_size

  ProofGeneration:
    describe(post_time, "Time to generate a PoSt Proof on a single core CPU", secs)
    describe(post_time_parallel, "Parallel time to generate a PoSt proof on RIG_CORES CPU cores", secs)
    describe(post_snark_time, "Time to generate a PoSt SNARK", secs)
    describe(post_snark_time_parallel, "Time to generate a PoSt SNARK in parallel", secs)
    describe(post_proof_gen, "Time to generate a PoSt Proof (including SNARKs, generating witnesses and reading from storage the challenged leaves)", secs)
    describe(post_proof_gen_parallel, "Time to generate a PoSt Proof (including SNARKs, generating witnesses and reading from storage the challenged leaves) in parallel", secs)
    describe(post_inclusions_time, "Time to generate merkle tree inclusion proofs for PoSt (assuming all data in memory)", secs)
    describe(post_inclusions_time_parallel, "Time to regenerate and to read the merkle tree inclusion proofs for the challenged nodes from memory in parallel", secs)

    post_time = post_proof_gen
    post_time_parallel = post_proof_gen_parallel
    post_proof_gen = post_snark_time + post_inclusions_time + post_data_access 
    post_proof_gen = post_snark_time_parallel + post_inclusions_time_parallel + post_data_access_parallel

    // TODO: add MerkleTreeCaching for ColumnR
    // This only accounts for generating the merkle trees
    InclusionProofs:
      Caching:
        describe(post_mtree_cached, "Size of the cached parts of the replica merkle tree", bytes)
        describe(post_mtree_cached_hashnodes, "Number of intermediary hash nodes cached of the replica merkle tree")
        describe(post_mtree_cached_gib, "POST_MTREE_CACHED in GiB")
        describe(post_mtree_overhead, "Fraction of extra storage a miner will have to use to cache the replica merkle tree")
        describe(post_mtree_layers_cached, "Number of layers of the replica merkle tree being cached")
        describe(post_mtree_layers_deleted, "Number of layers of the replica merkle tree not being cached")

        post_mtree_cached = post_mtree_cached_hashnodes * node_size
        post_mtree_cached_gib = post_mtree_cached / b_in_gib 
        post_mtree_overhead = post_mtree_cached / sector_size
        post_mtree_cached_hashnodes = expt(2, post_mtree_layers_cached) - 1
        post_mtree_layers_deleted = commr_tree_depth - post_mtree_layers_cached

      Generation:
        describe(post_mtree_regen_hashnodes, "Number of intermediary nodes to be regenerated for a single replica merkle tree inclusion proof in PoSt")
        describe(post_mtree_regen_nodes, "Number of leaf nodes to be access for a single replica merkle tree inclusion proof in PoSt")
        describe(post_inclusion_time, "Time to generate a Merkle tree inclusion proof for PoSt", secs)

        post_mtree_regen_hashnodes = expt(2, post_mtree_layers_deleted + 1) - 1
        // TODO: this is for a single inclusion, check if this is used in the right way
        post_mtree_regen_nodes = expt(2, post_mtree_layers_deleted) - 1
        post_inclusions_time = post_inclusion_time * post_challenges

      Generation (Wrapping):
        post_inclusion_time = (post_mtree_regen_hashnodes * 2) * merkle_tree_hash_time

      Generation (StackedReplicas):
        post_inclusion_time = (post_mtree_regen_hashnodes * 2) * merkle_tree_hash_time

      Generation (WrappingVariant):
        describe(wrapper_kdf_time, "Time to generate a node on the wrapper layer", secs)

        wrapper_kdf_time = kdf_time * wrapper_parents * post_mtree_regen_nodes
        post_inclusion_time = (post_mtree_regen_hashnodes * 2) * merkle_tree_hash_time + wrapper_kdf_time

  StorageAccess:
    describe(post_challenge_read, "Time to read from storage medium data required for a single challenge in PoSt", secs)
    describe(post_data_access, "Time to retrieve from the storage unit the data for PoSt", secs)
    describe(post_data_access_parallel, "Time to retrieve from the storage unit the data in parallel for PoSt", secs)

    post_data_access = post_challenge_read * post_challenges
    post_data_access_parallel = post_data_access / rig_storage_parallelization

    Access (Wrapping):
      // TODO: this is using random access, we should use seq access instead
      post_challenge_read = (1 + post_mtree_regen_nodes) * rig_storage_latency
      // TODO: log2(nodes) > post_mtree_layers_cached

    Access (WrappingVariant):
      describe(wrapper_lookup_read, "Time to retrieve from storage medium all the parents of a wrapper layer node", secs)
      describe(wrapper_lookup_with_mtree, "Time to retrieved from storage medium all the parents of the challenged node and those required for the merkle tree regeneration", secs)

      wrapper_lookup_read = wrapper_parents * rig_storage_latency
      wrapper_lookup_with_mtree = wrapper_lookup_read * (1 + post_mtree_regen_nodes)
      post_challenge_read = wrapper_lookup_with_mtree 

    Access (StackedReplicas):
      describe(stack_lookup_read, "Time to read from storage medium the column of a single window", secs)

      stack_lookup_read = windows * rig_storage_latency
      post_challenge_read = stack_lookup_read 

  SNARK:
    describe(inclusion_proof, "Constraints for checking an inclusion proof in a binary merkle tree as large as the sector")
    describe(post_inclusion_proofs, "Constraints for checking all inclusion proofs (in PoSt)")
    describe(ticket_proofs, "Constraints for checking the partial ticket (in PoSt)")
    describe(ticket_constraints, "Constraints of a partial ticket per challenged node (in PoSt)")
    describe(post_snark_constraints, "Constraints for a PoSt")
    describe(post_snark_partitions, "Number of partitions of a PoSt proof")
    describe(post_snark_size, "Size of a PoSt proof", bytes)
    describe(post_snark_size_kib, "Size of a Post proof", kib)
    describe(post_snark_partition_constraints, "Number of constraints for a PoSt partition")

    inclusion_proof = merkle_tree_hash_constraints * log2(nodes)
    post_inclusion_proofs = inclusion_proof * post_challenges
    ticket_proofs = ticket_constraints * post_challenges
    // TODO: handle different constructions
    post_snark_constraints = ticket_proofs + post_inclusion_proofs
    post_snark_partitions = post_snark_constraints / post_snark_partition_constraints
    post_snark_size = post_snark_partitions * snark_size
    post_snark_size_kib = post_snark_size / b_in_kib
    post_snark_time = snark_constraint_time * post_snark_constraints

ProofOfReplication:
  Security:
    describe(porep_challenges, "Number of challenges in a PoRep proof")
    describe(porep_lambda, "Bits of security of a PoRep proof")
    describe(spacegap, "Maximum difference in storage between an honest prover and a malicious prover")
    describe(porep_soundness, "Probability of an adversary to succeed in generating a convincing PoRep proof")

    porep_soundness = expt(2, -porep_lambda)

  Graph:
    describe(nodes, "Number of nodes in a sector")
    describe(sector_size, "Size of a sector", bytes)
    describe(sector_size_mib, "Size of a sector", mib)
    describe(sector_size_gib, "Size of a sector", gib)

    describe(window_nodes, "Number of nodes in a window")
    describe(window_size, "Size of a window", bytes)
    describe(window_size_mib, "Size of a window", mib)
    describe(windows, "Number of windows in a sector")

    describe(graph_parents, "Number of parents of a node in the PoRep window graph")
    describe(drg_parents, "Number of DRG parents of a node in the PoRep window graph")
    describe(expander_parents, "Number of Chung Expander parents of a node in the PoRep window graph")
    describe(stacked_layers, "Number of layers in the DRG")
    describe(wrapper_parents, "Number of parents in the wrapper layer")

    describe(node_size, "Size of a node", bytes)
    describe(node_size_mib, "Size of a node", mib)
    describe(node_size_gib, "Size of a node", gib)
    describe(kdf_hash_size, "Size of the input to the KDF hash", bytes)
    describe(kdf_hash_size_gib, "Size of the input to the KDF hash", gi)

    sector_size = sector_size_gib * b_in_gib
    sector_size_mib = sector_size / b_in_mib 
    graph_parents = drg_parents + expander_parents
    nodes = sector_size / node_size
    node_size_mib = node_size / b_in_mib 
    node_size_gib = node_size / b_in_gib 
    kdf_hash_size_gib = graph_parents * node_size_gib
    kdf_hash_size = kdf_hash_size_gib * b_in_gib

    window_size = window_size_mib * b_in_mib 
    windows = sector_size / window_size
    window_nodes = nodes / windows

  Time:
    describe(porep_time, "Time to seal a sector (PoRep proof)", secs)
    describe(porep_time_parallel, "Time to seal a sector (PoRep proof) with parallelization", secs)
    describe(porep_proof_gen, "Time to generate a PoRep proof (SNARK, witness generation, loading data from storage medium)", secs)
    describe(porep_proof_gen_parallel, "Time to generate a PoRep proof (SNARK, witness generation, loading data from storage medium) in parallel", secs)

    porep_time = porep_proof_gen + porep_commit_time + encoding_time
    porep_time_parallel = porep_proof_gen_parallel + porep_commit_time_parallel + encoding_time_parallel

  Size:
    describe(porep_proof_size, "Size of a PoRep proof")
    porep_proof_size = porep_snark_size + seal_onchain_commitments_size

  Commitment:
    describe(window_comm_tree_time, "Time to generate a merkle tree of stacked windows", secs)
    describe(window_comm_leaves_time, "Time to commit columns of a window", secs)

    describe(commit_size, "Size of a commitment", bytes)
    describe(commr_size, "Size of CommR", bytes)
    describe(commd_size, "Size of CommD", bytes)
    describe(commrlast_size, "Size of CommRLast", bytes)

    describe(commr_tree_depth, "Number of layers of the merkle tree with root hash CommR")

    describe(seal_onchain_commitments_size, "Size of the seal commitments", bytes)
    describe(porep_commit_time, "Time to generate CommC, CommR and CommQ", secs)
    describe(porep_commit_time_mins, "Time to generate CommC, CommR and CommQ", mins)
    describe(porep_commit_time_parallel, "Time to generate CommC, CommR and CommQ in parallel", secs)


    describe(commr_time, "Time to generate CommR", secs)
    describe(commd_time, "Time to generate CommD", secs)
    describe(commc_time, "Time to generate CommC", secs)

    commit_size = cid_size
    // TODO: should be missing *2
    window_comm_tree_time = merkle_tree_hash_time * (window_nodes * 2 - 1)
    window_comm_leaves_time = column_leaf_hash_time * windows
    commr_size = commit_size
    commd_size = commit_size
    commrlast_size = commit_size

    seal_onchain_commitments_size = commd_size + commr_size
    // seal_commitment_time = commr_time

    commd_time = merkle_tree_datahash_time * (2 * nodes - 1)

    // TODO commr might be twice depending on CommQ
    porep_commit_time = commr_time + commc_time
    porep_commit_time_mins = porep_commit_time / 60

    CommR (VectorR):
      commr_time = merkle_tree_hash_time * nodes
      commr_tree_depth = log2(nodes) 

    CommR (ColumnR):
      commr_time = window_comm_tree_time + window_comm_leaves_time 
      commr_tree_depth = log2(window_nodes)

    CommC:
      describe(commc_tree_time, "Time to generate the merkle tree of CommC (after the leaves have been computed)", secs)
      commc_tree_time = (window_comm_tree_time + window_comm_leaves_time) * stacked_layers 

  Encoding:
    describe(encoding_time, "Time to encode a sector", secs)
    describe(encoding_time_parallel, "Time to encode a sector in parallel", secs)
    describe(encoding_time_asic, "Time to encode a sector with an ASIC", secs)
    describe(encoding_time_parallel_asic, "Time to encode a sector in parallel with ASICs", secs)
    describe(encoding_window_time, "Time to encode a window", secs)
    describe(encoding_window_time_parallel, "Time to encode a window in parallel", secs)
    describe(encoding_window_time_asic, "Time to encode a window with an ASIC", secs)
    describe(encoding_window_time_parallel_asic, "Time to encode a window in parallel with ASICs", secs)
    describe(kdf_time, "Time to compute a KDF", secs)

    encoding_time = encoding_window_time * windows
    encoding_time_parallel = encoding_time / rig_cores

    encoding_window_time = (window_nodes * graph_parents) * kdf_time * stacked_layers
    encoding_window_time_parallel = encoding_window_time / rig_cores

    Asic:
      encoding_time_parallel_asic = encoding_time_parallel / hashing_amax
      encoding_time_asic = encoding_time / hashing_amax
      encoding_window_time_asic = encoding_window_time / hashing_amax
      encoding_window_time_parallel_asic = encoding_window_time_parallel / hashing_amax

  Retrieval:
    describe(window_read_time, "Time to read a window from storage unit", secs)
    describe(window_read_time_parallel, "Time to read a window from storage unit in parallel", secs)
    describe(decoding_time, "Time to decode a sector", secs)
    describe(decoding_time_parallel, "Time to decode a sector in parallel", secs)

    Decoding (StackedReplicas):
      // Decoding in StackedReplicas: read the window output and decode it
      window_read_time = window_size_mib / rig_storage_read_mibs
      window_read_time_parallel = window_read_time / rig_storage_parallelization
      decoding_time = encoding_window_time + window_read_time
      decoding_time_parallel = encoding_window_time_parallel + window_read_time_parallel

    Decoding (WrappingVariant):
      // Decoding in WrappingVariant: read the window output and decode them
      window_read_time = window_size_mib / rig_storage_read_mibs
      window_read_time_parallel = window_read_time / rig_storage_parallelization
      decoding_time = encoding_window_time + window_read_time
      decoding_time_parallel = encoding_window_time_parallel + window_read_time_parallel

    Decoding (Wrapping):
      // Decoding in Wrapping: read all the window outputs and decode them
      window_read_time = sector_size_mib / rig_storage_read_mibs
      window_read_time_parallel = window_read_time / rig_storage_parallelization
      decoding_time = encoding_time + window_read_time
      decoding_time_parallel = encoding_time_parallel + window_read_time_parallel

  SNARK:
    describe(porep_snark_constraints, "Constraints for a PoRep")
    describe(porep_snark_partitions, "Number of partitions of a PoRep proof")
    describe(porep_snark_time, "Time to generate a PoRep SNARK proof", secs)
    describe(porep_snark_size, "Size of a PoRep proof", bytes)
    describe(porep_snark_size_kib, "Size of a PoRep proof", kib)
    describe(porep_snark_partition_constraints, "Number of constraints for a PoRep partition")

    describe(kdf_constraints, "Constraints of KDF per node")
    describe(labeling_proof, "Constraints to check labeling of a single node")
    describe(labeling_proofs, "Constraints to check labeling of a window")
    describe(window_labeling_proof, "Constraints to check correct labeling for all windows")
    describe(data_inclusion, "Constraints to check a single inclusion in CommD")
    describe(data_inclusions, "Constraints to check all the inclusions required in a window in CommD")
    describe(window_data_inclusions,  "Constraints to check all the inclusions required for all windows in CommD")
    describe(replica_inclusion, "Constraints to check a single inclusion in CommR")
    describe(replica_inclusions, "Constraints to check all the inclusions required in a window in CommR")
    describe(window_replica_inclusions,  "Constraints to check all the inclusions required for all windows in CommR")
    describe(window_inclusion, "Constraints to check a single inclusion proof in a window")
    describe(window_inclusions, "Constraints to check all the required inclusion proofs in a window")
    describe(column_leaf, "Constraints to check a single leaf generation from a column in a window")
    describe(column_leaves, "Constraints to check all the leaves generation required in a window")
    describe(window_column_leaves, "Constraints to check all the leaves generation required for all windows")

    // D
    // Data layer committed with merkle tree
    data_inclusion = merkle_tree_datahash_constraints * log2(nodes)
    data_inclusions = data_inclusion * porep_challenges
    window_data_inclusions = data_inclusion * porep_challenges * windows

    // C
    window_inclusion = merkle_tree_hash_constraints * log2(window_nodes)
    window_inclusions =  window_inclusion * porep_challenges * (graph_parents + 1)
    column_leaf = column_leaf_hash_constraints * stacked_layers
    column_leaves = column_leaf * porep_challenges * (graph_parents + 1)
    window_column_leaves = column_leaves * windows
    labeling_proof = kdf_constraints * graph_parents
    labeling_proofs = labeling_proof * stacked_layers * porep_challenges
    window_labeling_proof = labeling_proofs * windows

    // Q and R
    replica_inclusion = merkle_tree_hash_constraints * log2(nodes)
    replica_inclusions = replica_inclusion * porep_challenges
    window_replica_inclusions = replica_inclusions * windows

    porep_snark_partitions = porep_snark_constraints / porep_snark_partition_constraints
    porep_snark_size = porep_snark_partitions * snark_size
    porep_snark_size_kib = porep_snark_size / b_in_kib
    porep_snark_time = snark_constraint_time * porep_snark_constraints

    Constraints (WindowEncoding):
      porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + window_data_inclusions + window_replica_inclusions*2

    Constraints (SectorEncoding):
      porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + data_inclusions + replica_inclusions*2

    Constraints (Wrapping):
      porep_snark_constraints = window_inclusions + window_column_leaves + window_labeling_proof + data_inclusions + replica_inclusions*2

    // Column comitting data or replica. 

ProofOfReplication (StackedChungParameters):
  describe(chung_a, "Chung A")
  describe(chung_c, "Chung C")
  describe(chung_delta, "Chung delta")
  describe(chung_sigma, "Chung sigma")

  chung_a = spacegap - (chung_c / (chung_c -1)) * chung_delta + ((chung_c-1)/ chung_c) * (chung_sigma - 1)

  stacked_layers = (log2(regeneration_fraction / chung_a) / log2(chung_c)) + 1
  porep_challenges = -porep_lambda / (log2(1-chung_delta))
  post_challenges = -post_lambda / (log2(chung_sigma))

EC:
  describe(ec_e, "Number of winning miners per epoch on expectation")

Filecoin:
  describe(filecoin_block_time, "Time between two blocks", secs)
  describe(blocks_in_a_year, "Number of blocks in a year")

  filecoin_block_time = polling_time
  blocks_in_a_year = (secs_in_year / filecoin_block_time) * ec_e

  Storage:
    describe(filecoin_sectors, "Number of sectors in the Filecoin DSN")
    describe(filecoin_miners, "Number of Storage Miners")
    describe(filecoin_storage_capacity, "Amount of storage that the Filecoin DSN can support", bytes)
    describe(filecoin_storage_capacity_gib, "Amount of storage that the Filecoin DSN can support", gib)
    describe(filecoin_storage_capacity_eib, "Amount of storage that the Filecoin DSN can support", eib)

    filecoin_sectors = filecoin_storage_capacity_gib / sector_size_gib
    filecoin_storage_capacity_eib = filecoin_storage_capacity / b_in_eib
    filecoin_storage_capacity_gib = filecoin_storage_capacity / b_in_gib

  Chain:
    describe(chain_size_year, "Size of one year worth of blocks", bytes)
    describe(chain_size_year_gib, "Size of one year worth of blocks", kib)
    describe(block_size, "Size of a block", bytes)
    describe(block_size_kib, "Size of a block", bytes)
    describe(proofs_block_fraction, "Fraction of the block dedicated to proofs")

    chain_size_year = block_size * blocks_in_a_year
    chain_size_year_gib = chain_size_year / b_in_gib
    block_size_kib = block_size / b_in_kib

    // Note this is simplistic and accounts for no framing
    block_size = (proofs_per_block / proofs_block_fraction) 

  Proofs: 
    describe(seals_per_year, "Number of PoRep proofs per year")
    describe(posts_per_year, "Number of PoSt proofs per year")
    describe(filecoin_reseals_per_year, "Number of times a sector is re-sealed in a year")
    describe(seals_per_sector_per_year, "Number of seal proofs per sector per year")
    describe(posts_per_sector_per_year, "Number of post proofs per sector per year")
    describe(seals_size_per_year, "Size of seal proofs per year", bytes)
    describe(posts_size_per_year, "Size of post proofs per year", bytes)
    describe(seals_size_per_block, "Size of seal proofs per block", bytes)
    describe(posts_size_per_block, "Size of post proofs per block", bytes)
    describe(proofs_per_block, "Size of proofs per block", bytes)

    seals_per_sector_per_year = filecoin_reseals_per_year + 1
    seals_per_year = filecoin_sectors * seals_per_sector_per_year
    posts_per_year = filecoin_sectors * posts_per_sector_per_year 

    seals_size_per_year = seals_per_year * porep_proof_size
    posts_size_per_year = posts_per_year * post_proof_size

    seals_size_per_block = seals_size_per_year / blocks_in_a_year
    posts_size_per_block = posts_size_per_year / blocks_in_a_year

    proofs_per_block = seals_size_per_block + proofs_per_block

    PoSt (ElectionWithFallbackPoSt):
      describe(fposts_per_sector_per_year, "Number of fallback post proofs per sector per year")
      describe(eposts_per_sector_per_year, "Number of election post proofs per sector per year")
      describe(fallback_ratio, "Fraction of fallback PoSts")
      describe(fallback_period, "Time between last proof submitted and a required fallback PoSt", secs)
      posts_per_sector_per_year = eposts_per_sector_per_year + fposts_per_sector_per_year

      // TODO: model single ticket vs multiple tickets
      ElectionPoSt:
        eposts_per_sector_per_year = blocks_in_a_year / filecoin_sectors

      FallbackPoSt:
        fposts_per_sector_per_year = (secs_in_year / fallback_period) * fallback_ratio

  Mining:

Rationality:
  describe(breakeven_nodes, "number of nodes / sector / challenge period after which it's cheaper to store the entire sector, than re-encode those")
  describe(polling_time, "Time between two PoSt proofs", secs)
  describe(challenge_periods_per_month, "Number of challenge periods in a month")
  describe(regeneration_fraction, "Fraction of nodes of a sector for which re-encoding them every `polling_time` costs as much as storing them")

  breakeven_nodes = ((cost_gib_per_month *  sector_size_gib) / (cost_node_encoding * challenge_periods_per_month))
  challenge_periods_per_month = secs_in_month / polling_time
  regeneration_fraction = breakeven_nodes / nodes

  Costs:
    Compute:
      describe(cost_node_encoding, "Cost of encoding a single node")
      describe(nodes_per_second, "Nodes encoded per second using the fastest hardware")
      describe(nodes_per_year, "Nodes encoded per year using the fastest hardware")
      describe(hash_gb_per_second, "GB that can be hashed per second")
      describe(hash_gib_per_second, "GiB that can be hashed per second")

      hash_gib_per_second = hash_gb_per_second * to_i
      nodes_per_second = hash_gib_per_second / kdf_hash_size_gib
      nodes_per_year = nodes_per_second * secs_in_year 
      cost_node_encoding = rig_malicious_cost_per_year / nodes_per_year 

    Storage:
      describe(cost_gb_per_month, "Cost of storing 1GB per month")
      describe(cost_gib_per_month, "Cost of storing 1GiB per month")
      describe(cost_storage_gibs, "Cost of storing 1GiB per second")
      describe(cost_storage_sector_polling_time, "Cost of storage for `polling_time`")
      describe(extra_storage_time, "Time for which an honest user must be storing a file beyond the challenge period")

      cost_gib_per_month = cost_gb_per_month * to_i
      cost_storage_gibs = cost_gib_per_month / secs_in_month 
      cost_storage_sector_polling_time = cost_storage_gibs * (sector_size_gib * (polling_time + extra_storage_time))